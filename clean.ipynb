{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import hashlib\n",
    "import os\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "\n",
    "from icecream import ic\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "root_path = Path(os.getcwd())  # TODO Change it to your Path\n",
    "root_path"
   ],
   "id": "63e0831f9363c953",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:23:22.512336Z",
     "start_time": "2024-12-20T01:23:22.509203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_file_hash(path) -> str:\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(2 ** 13), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ],
   "id": "371ddc79c6f8179f",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:23:22.933058Z",
     "start_time": "2024-12-20T01:23:22.929389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_and_print(results):\n",
    "    filtered_results = dict(\n",
    "        filter(\n",
    "            lambda result:\n",
    "            len(result[1]) > 1 and\n",
    "            result[1][0].stat().st_size > 0,\n",
    "            results.items()\n",
    "        )\n",
    "    )\n",
    "    for _, selected_paths in filtered_results.items():\n",
    "        ic(selected_paths)\n"
   ],
   "id": "596cd645aea5b046",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:23:23.928198Z",
     "start_time": "2024-12-20T01:23:23.863016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paths: list = [\n",
    "    path for path in root_path.glob(\"**/*\")\n",
    "    if path.is_file() and\n",
    "       not any([parent.name.startswith(\".\") for parent in path.parents]) and\n",
    "       not path.name.startswith(\".\")\n",
    "]"
   ],
   "id": "5b1c169e8efbe514",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Naive Solution (Slow)\n",
    "\n",
    "1. Hash every file\n",
    "2. remove all groups with one file\n",
    "3. print out dublicates"
   ],
   "id": "f6b8f57e8d55b96e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "for path in tqdm(paths):\n",
    "    p_hash = get_file_hash(path)\n",
    "    if p_hash not in results:\n",
    "        results[p_hash] = []\n",
    "    results[p_hash] += [path]\n",
    "\n",
    "filter_and_print(results)"
   ],
   "id": "cdb1ab6c3bddbef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Complex Solution (Fast)\n",
    "\n",
    "1. Sort by size\n",
    "2. Group by size (files that have the same content also have the same size)\n",
    "3. Group by suffix (we assume that suffix is the same and name could be changed)\n",
    "4. Remove all groups with one file\n",
    "5. Calculate hash and print all found dublicates\n",
    "\n",
    "Step 1-4 are cheap and reduce the amount of files that are hashed"
   ],
   "id": "9f8e3e3df3371a98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:23:36.085239Z",
     "start_time": "2024-12-20T01:23:36.017994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sorted_paths = sorted(paths, key=lambda path: path.stat().st_size)\n",
    "\n",
    "grouped_by_size = groupby(sorted_paths, key=lambda path: path.stat().st_size)\n",
    "size_to_paths_map = {size: list(paths_group) for size, paths_group in grouped_by_size}\n",
    "duplicate_size_groups = list(\n",
    "    filter(lambda group: len(group[1]) > 1, size_to_paths_map.items())\n",
    ")\n",
    "\n",
    "hash_to_paths_map = {}\n",
    "\n",
    "for _, paths_with_same_size in tqdm(duplicate_size_groups):\n",
    "    for file_path in paths_with_same_size:\n",
    "        file_hash = get_file_hash(file_path)\n",
    "        if file_hash not in hash_to_paths_map:\n",
    "            hash_to_paths_map[file_hash] = []\n",
    "        hash_to_paths_map[file_hash].append(file_path)\n",
    "\n",
    "filter_and_print(hash_to_paths_map)"
   ],
   "id": "e8d0404f70f42f13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 141.52it/s]\n"
     ]
    }
   ],
   "execution_count": 321
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf7716cd935c77f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
